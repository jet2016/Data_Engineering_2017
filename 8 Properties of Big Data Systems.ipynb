{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Data Engineering final project is designed to include all 8 properties of a Big Data System. \n",
    "\n",
    "We will include the definition of each of those properties and how this end-to-end Big Data system identifies and addresses it.\n",
    "\n",
    "The website below defines these 8 properties and I will be defining them from here and then sharing how my project addresses these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness and fault Tolerance\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Defining the property above:\n",
    "\n",
    "\"Building systems that \"do the right thing\" is difficult in the face of the challenges of distributed systems. Systems need to behave correctly despite machines going down randomly, the complex semantics of consistency in distributed databases, duplicated data, concurrency, and more. These challenges make it difficult even to reason about what a system is doing. Part of making a Big Data system robust is avoiding these complexities so that you can easily reason about the system.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"It's imperative for systems to be human-fault tolerant. This is an oft-overlooked property of systems that we're not going to ignore. In a production system, it's inevitable that someone will make a mistake sometime, such as by deploying incorrect code that corrupts values in a database. If you build immutability and recomputation into the core of a Big Data system, the system will be innately resilient to human error by providing a clear and simple mechanism for recovery.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the systems, concepts and integration modalities we learned in the Data Engineering class, all the systems used in this project mesh and integrate with one another very well.  \n",
    "\n",
    "1) For streaming the data from data source - Pitchfork in this case, we use EC2 to stream and S3 to store, which are both fault tolerant.\n",
    "\n",
    "2) All systems used for the project neatly integrates with one another, with good Robustness.\n",
    "\n",
    "3) EMR cluster is where we do all the work or computing. EMR cluster is used to send data to data base - AWS RDS and PostGres, which ensures data is immutable.\n",
    "\n",
    "4) Spark is used to do all computing which again ensures that system is robust and can handle large amounts of data.\n",
    "\n",
    "5) Spark MLlib is used to perform word count and Naive Bayes on dataset in RDS - which ensures that system is going to behave correctly\n",
    "\n",
    "6) All the raw data webscraped from Pitchfork is stored in S3. In case some system fails, the data can still be used to recompute the desired results.\n",
    "\n",
    "7)I may look into using Elastic Beanstalk to deploy fresh EC2 instances in the event that one fails.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low latency reads and updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n",
    "\n",
    "\"The vast majority of applications require reads to be satisfied with very low latency, typically between a few milliseconds to a few hundred milliseconds. On the other hand, the update latency requirements vary a great deal between applications. Some applications require updates to propagate immediately, but in other applications a latency of a few hours is fine. Regardless, you need to be able to achieve low latency updates when you need them in your Big Data systems. More importantly, you need to be able to achieve low latency reads and updates without compromising the robustness of the system.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Webscraping using Beautiful Soup is done to store the data in AWS S3 and reading this data off of S3 ensures low latency.\n",
    "\n",
    "2) When it comes to the analysis of historical surge data, Latency is not very important. But, using spark, it will lower latency.\n",
    "\n",
    "3)In future I will create Apache Parquet files which will make data processing in Spark faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n",
    "\n",
    "\"Scalability is the ability to maintain performance in the face of increasing data or load by adding resources to the system. The Lambda Architecture is horizontally scalable across all layers of the system stack: scaling is accomplished by adding more machines.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) EMR cluster ensures scalability.\n",
    "\n",
    "2) I use Apache Spark for my computations in my project which helps in scalability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n",
    "\n",
    "\"A general system can support a wide range of applications. Because the Lambda Architecture is based on functions of all data, it generalizes to all applications, whether financial management systems, social media analytics, scientific applications, social networking, or anything else.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) All file states (raw, interim, processed) are stored in AWS S3\n",
    "\n",
    "2) I would like to build and use Airflow to coordinate my Spark DAGs. This would enable projects with prediction and machine learning. It would be nice to enable queries in the web app that trigger a Spark job that returns an answer. Elastic Search may be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n",
    "\n",
    "\"You don't want to have to reinvent the wheel each time you add a related feature or make a change to how your system works. Extensible systems allow functionality to be added with a minimal development cost. Often a new feature or a change to an existing feature requires a migration of old data into a new format. Part of making a system extensible is making it easy to do large-scale migrations. Being able to do big migrations quickly and easily is core to the approach you'll learn.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Saving my raw files onto S3 and extensive modularization of Spark code makes this system somewhat extensible.\n",
    "\n",
    "2) Work on Makefiles and automate the environment setup would be steps I would like to take.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad hoc queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n",
    "\n",
    "\"Being able to do ad hoc queries on your data is extremely important. Nearly every large dataset has unanticipated value within it. Being able to mine a dataset arbitrarily gives opportunities for business optimization and new applications. Ultimately, you can't discover interesting things to do with your data unless you can ask arbitrary questions of it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Need to login(ssh) to Spark EMR to do ad-hoc processing of data in RDS. \n",
    "\n",
    "2) Elastic Search combined with Spark would be a future improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal maintainance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n",
    "\n",
    "\"Maintenance is a tax on developers. Maintenance is the work required to keep a system running smoothly. This includes anticipating when to add machines to scale, keeping processes up and running, and debugging anything that goes wrong in production.\n",
    "\n",
    "An important part of minimizing maintenance is choosing components that have as little implementation complexity as possible. You want to rely on components that have simple mechanisms underlying them. In particular, distributed databases tend to have very complicated internals. The more complex a system, the more likely something will go wrong, and the more you need to understand about the system to debug and tune it.\n",
    "\n",
    "You combat implementation complexity by relying on simple algorithms and simple components. A trick employed in the Lambda Architecture is to push complexity out of the core components and into pieces of the system whose outputs are discardable after a few hours. The most complex components used, like read/write distributed databases, are in this layer where outputs are eventually discardable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) As all the components are maintained AWS, so less maintenance. EMR and RDS require  minimize maintenance.\n",
    "\n",
    "2) EC2 instances need maintenance. Probably EBS might help improve the scenario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debuggability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.developer.com/db/desired-properties-of-a-big-data-system.html\n",
    "\n",
    "\"A Big Data system must provide the information necessary to debug the system when things go wrong. The key is to be able to trace, for each value in the system, exactly what caused it to have that value.\n",
    "\n",
    "\"Debuggability\" is accomplished in the Lambda Architecture through the functional nature of the batch layer and by preferring to use recomputation algorithms when possible.\n",
    "\n",
    "Achieving all these properties together in one system may seem like a daunting challenge. But by starting from first principles, as the Lambda Architecture does, these properties emerge naturally from the resulting system design.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This project was designed to satisfy this property. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Emails to notify myself the amount/count of data that I have collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2) Heavy use of logging module.\n",
    "\n",
    "3) Store all the logs on S3.\n",
    "\n",
    "4) Airflow would help with debuggability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
