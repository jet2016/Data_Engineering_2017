{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Steps to do Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entire project is based on gathering reviews from Pitchfork: http://pitchfork.com/\n",
    "* more info at \"Overview.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the script \"pf_miner.py\"\n",
    "* This script webscrapes the Pitchfork Reviews page (http://pitchfork.com/reviews/albums/)       using Beautiful Soup\n",
    "* Using boto it connects to S3 and dumps the review data in json format\n",
    "* Use the cronjob to run every 24 hours at 6:15pm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed steps on how I Stream the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Please note, I am trying to run the code in Notebook here to make it look modular. The reason I am running into errors here is I have put some of the functions and other files in separate folders for proper book keeping. So from this location I would have to change the paths of the functions I am calling. Recommended way to run this--- All the scripts will run in PyCharm or Atom. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/spark-submit\n",
    "\n",
    "from os.path import expanduser\n",
    "import requests #when we type in a link in browser we make a request to that link\n",
    "#we are using request to fetch information from link, once we get it we feed it to soup\n",
    "#Soup parses html and gives us a tree and we go in there and get the information out of it \n",
    "#using soup\n",
    "#This is how we are doing webscraping, as shown below. \n",
    "import json\n",
    "import yaml\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse as date_parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from make_db import partition_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The base url is the pitchfork reviews page and it returns review columns as a list\n",
    "pf_url = 'http://pitchfork.com'\n",
    "base_url = 'http://pitchfork.com/reviews/albums/'\n",
    "review_columns = ['artist','album','score', 'album_art','genre','label',\n",
    "                'pub_date','abstract','featured_tracks', 'review_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pitchfork reviews](images/Scratch_Pitchfork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  Here we are taking data and storing it in S3, so connecting to S3. \n",
    "\n",
    "**  This part is part of the second S - Storing, but it is inside code so we will briefly talk about it here.\n",
    "\n",
    "**  We connect to AWS S3 with our AWS credentials and *.yaml file which stores them\n",
    "\n",
    "**  We create a bucket in AWS, here the name is \"finalprojectpitchfork.\" \n",
    "\n",
    "**  We upload our data each time we get a review using the function \"upload_to_s3\"---include Pitchfork review albums - this page contains links to each of the reviews\n",
    "\n",
    "**  We fetch the links from here and then fetch the actual review page using the links\n",
    "and each individual review page contains information we want. \n",
    "\n",
    "**  Once we check that this code is running on local machine I created an EC2 instance on AWS to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_to_s3(credentials, bucket_name='finalprojectpitchfork'):\n",
    "\n",
    "    conn = S3Connection(credentials['aws']['access_key_id'],credentials['aws']['secret_access_key'])\n",
    "\n",
    "    try:\n",
    "        bucket = conn.get_bucket(bucket_name)\n",
    "        print(\"Bucket found on S3\")\n",
    "    except:\n",
    "        bucket = conn.create_bucket(bucket_name)\n",
    "        print(\"Creating bucket\")\n",
    "    return conn, bucket\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, data):\n",
    "\n",
    "    try:\n",
    "        k = Key(bucket)\n",
    "        k.key = data['artist'] + ' - ' + data['album']\n",
    "        k.set_contents_from_string(json.dumps(data, ensure_ascii=False))\n",
    "        print(\"Uploaded \", k.key)\n",
    "    except:\n",
    "        print(\"Failed to upload\", k.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting number of reviews to be fetched to be 10,000\n",
    "#Request limit is set to 20. If there is error handling, i.e. either the site blocks us or\n",
    "#there are no more reviews, we set request limit to 20.\n",
    "def pf_miner(credentials, limit=10000, req_lim=20):\n",
    "\n",
    "    err_count = 0\n",
    "\n",
    "    _, bucket = connect_to_s3(credentials)\n",
    "\n",
    "    for page in range(1, limit):\n",
    "        print(\"Mining page: \", page)\n",
    "\n",
    "        info_list = req_parse(base_url+'?page={}'.format(page),\n",
    "                req_lim).findAll(\"div\", {\"class\": \"review\"})\n",
    "# when we request a webpage it takes a lot of waiting time, instead of waiting for them one at a time\n",
    "#we request multiple ones. it takes 2 hours to get all of them this way, otherwise it takes overnight.\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "\n",
    "            future_to_url = {executor.submit(req_task, pf_url+info.a['href'],\n",
    "                    req_lim): info.a['href'] for info in info_list}\n",
    "\n",
    "#as completed dictionary gets fed in url dictionary\n",
    "# yields task as they are completed\n",
    "            for future in as_completed(future_to_url):\n",
    "                try:\n",
    "                    # get the resulting data\n",
    "                    data = future.result()\n",
    "#Either something wrong with the review or we are out of reviews. \n",
    "#Thats why I set a threshold of 20 errors.\n",
    "                except (AttributeError, TypeError):\n",
    "\n",
    "                    err_count += 1\n",
    "                    if err_count >= req_lim:\n",
    "                        print('No more reviews found. Exiting...')\n",
    "                        return\n",
    "\n",
    "                    print('No more links on the page.')\n",
    "                    continue\n",
    "                except:\n",
    "                    print('Unknown Error occurred: ', future.exception())\n",
    "                    continue\n",
    "\n",
    "                upload_to_s3(bucket, data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1 request task gets one of the review pages with 1 review on it.  \n",
    "def req_task(link, req_lim):\n",
    "    '''\n",
    "    calls all the functions and returns a tuple of all the review data\n",
    "    '''\n",
    "    print('req_task: ', link)\n",
    "    soup = req_parse(link, req_lim)\n",
    "\n",
    "    review = [get_artist(soup), get_album(soup), get_score(soup),\n",
    "                get_album_art(soup), get_genres(soup), get_label(soup),\n",
    "                get_pub_date(soup), get_abstract(soup),\n",
    "                get_featured_tracks(soup), get_review_content(soup)]\n",
    "# originally the review gives a list of stuff as above and we convert it to json for easier handling\n",
    "# Alternatively we could have done it through saving it as *.csv but handling would have \n",
    "#been difficult\n",
    "\n",
    "\n",
    "#returns 1 page as json each time\n",
    "    my_json = {}\n",
    "    for col_name, item in zip(review_columns, review):\n",
    "        my_json[col_name] = ''.join([c for c in item if ord(c) < 128])\n",
    "\n",
    "    return my_json\n",
    "\n",
    "#handles the list of multiple reviews page and return the soup of it\n",
    "\n",
    "def req_parse(link, req_lim):\n",
    "    '''\n",
    "    GET the link, returns None if failure. Returns the soup if all went well.\n",
    "    '''\n",
    "    for _ in range(req_lim):\n",
    "        try:\n",
    "            req = requests.get(link,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            break\n",
    "        except:\n",
    "            print('stuck')\n",
    "            sleep(5)\n",
    "\n",
    "    # Check the page and a catastrophe check \n",
    "    if req is None:\n",
    "        return\n",
    "\n",
    "    if req.status_code == 404:\n",
    "        print ('There are no more pages to mine. Breaking out of loop.')\n",
    "        return\n",
    "\n",
    "    # Parse html\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extractors go here\n",
    "def get_artist(soup):\n",
    "    try:\n",
    "        return soup.findAll('h2',{'class':'artists'})[0].text.replace(\":\", \" \").replace(\"/\", \" \")\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_album(soup):\n",
    "    try:\n",
    "        return soup.findAll('h1', {'class':'review-title'})[0].text.replace(\":\", \" \").replace(\"/\", \" \")\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_score(soup):\n",
    "    try:\n",
    "        return soup.findAll('span',{'class':'score'})[0].text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_album_art(soup):\n",
    "    try:\n",
    "        return soup.findAll('div',{'class':'album-art'})[0].img['src']\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_genres(soup):\n",
    "    try:\n",
    "        genres = soup.findAll('ul',{'class':'genre-list before'})\n",
    "        result = ''\n",
    "        for i in range(len(genres)):\n",
    "            result += genres[i].text\n",
    "        return result\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_label(soup):\n",
    "    try:\n",
    "        return soup.findAll('ul',{'class':'label-list'})[0].text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_pub_date(soup):\n",
    "    try:\n",
    "        return soup.findAll('span',{'class':'pub-date'})[0]['title']\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_abstract(soup):\n",
    "    try:\n",
    "        return soup.findAll('div',{'class':'abstract'})[0].text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_featured_tracks(soup):\n",
    "    try:\n",
    "        return soup.findAll('div',{'class':'player-display'})[0].text\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def get_review_content(soup):\n",
    "    try:\n",
    "        return soup.findAll('div', {'class':'contents dropcap'})[0].text.replace('\\n',' ')\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#checks the first page to see if there are any new reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is the cron job that with cron tab runs pf_minor.py every 24 hours at 6:15pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pf_croner(credentials, req_lim=20):\n",
    "    ''' checks the first page for new reviews '''\n",
    "\n",
    "    _, bucket = connect_to_s3(credentials)\n",
    "\n",
    "    info_list = req_parse(base_url+'?page=1', req_lim) \\\n",
    "            .findAll(\"div\", {\"class\": \"review\"})\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    # if the date cannot be parsed, that means it is new (e.g. \"23 hrs ago\")\n",
    "    for info in info_list:\n",
    "        data = req_task(pf_url+info.a['href'], req_lim)\n",
    "        \n",
    "        d = date_parse(data['pub_date'])\n",
    "        datediff = relativedelta(d, pytz.utc.localize(datetime.now()))\n",
    "        print(datetime.now(), d)\n",
    "        if datediff.years == 0 and datediff.months == 0 and datediff.days == 0:        \n",
    "            print(\"New Review Found\")\n",
    "            upload_to_s3(bucket, data)\n",
    "            data_list.append(data)\n",
    "\n",
    "    partition_job(data_list, credentials)\n",
    "\n",
    "    print(\"CRON Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "def partition_job(source, credentials=None):\n",
    "\n",
    "    if not credentials:\n",
    "        credentials = my_cred.value\n",
    "\n",
    "    conn, cur = connect_to_psql(credentials)\n",
    "\n",
    "    query = \"\"\"INSERT INTO reviews (artist, album, score, album_art, genre, label,\n",
    "                pub_date, abstract, review_content)\n",
    "                VALUES (%(artist)s, %(album)s, %(score)s, %(album_art)s, %(genre)s,\n",
    "                %(label)s, %(pub_date)s, %(abstract)s, %(review_content)s)\"\"\"\n",
    "\n",
    "    print(\"Starting job...\")\n",
    "\n",
    "    for item in source:\n",
    "\n",
    "        try:\n",
    "            if type(item) == unicode:\n",
    "                data = json.loads(item)\n",
    "            elif type(item) == dict:\n",
    "                data = item\n",
    "            print('Inserting album: ', data['album'])\n",
    "            cur.execute(query, data)\n",
    "        except:\n",
    "            conn.rollback()\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"Partition job completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket found on S3\n",
      "req_task:  http://pitchfork.com/reviews/albums/23135-world-spirituality-classics-1-the-ecstatic-music-of-alice-coltrane-turiyasangitananda/\n",
      "2017-05-10 16:14:28.390960 2017-05-10 05:00:00+00:00\n",
      "New Review Found\n",
      "Uploaded  Alice Coltrane - World Spirituality Classics 1  The Ecstatic Music of Alice Coltrane Turiyasangitananda\n",
      "req_task:  http://pitchfork.com/reviews/albums/23123-white-knight/\n",
      "2017-05-10 16:14:28.951958 2017-05-10 05:00:00+00:00\n",
      "New Review Found\n",
      "Uploaded  Todd Rundgren - White Knight\n",
      "req_task:  http://pitchfork.com/reviews/albums/23152-real-high/\n",
      "2017-05-10 16:14:29.393570 2017-05-10 05:00:00+00:00\n",
      "New Review Found\n",
      "Uploaded  Nite Jewel - Real High\n",
      "req_task:  http://pitchfork.com/reviews/albums/23217-finding-people-ep/\n",
      "2017-05-10 16:14:29.752114 2017-05-10 05:00:00+00:00\n",
      "New Review Found\n",
      "Uploaded  Croatian Amor - Finding People EP\n",
      "req_task:  http://pitchfork.com/reviews/albums/23099-inter-alia/\n",
      "2017-05-10 16:14:30.241413 2017-05-09 05:00:00+00:00\n",
      "req_task:  http://pitchfork.com/reviews/albums/23124-halo/\n",
      "2017-05-10 16:14:30.516306 2017-05-09 05:00:00+00:00\n",
      "req_task:  http://pitchfork.com/reviews/albums/23120-best-troubador/\n",
      "2017-05-10 16:14:30.794491 2017-05-09 05:00:00+00:00\n",
      "req_task:  http://pitchfork.com/reviews/albums/23129-trumpeting-ecstasy/\n",
      "2017-05-10 16:14:31.068693 2017-05-09 05:00:00+00:00\n",
      "req_task:  http://pitchfork.com/reviews/albums/23254-slowdive/\n",
      "2017-05-10 16:14:31.349345 2017-05-08 05:00:00+00:00\n",
      "req_task:  http://pitchfork.com/reviews/albums/23164-satans-graffiti-or-gods-art/\n",
      "2017-05-10 16:14:31.741611 2017-05-08 05:00:00+00:00\n",
      "req_task:  http://pitchfork.com/reviews/albums/23095-joan-shelley/\n",
      "2017-05-10 16:14:32.078751 2017-05-08 05:00:00+00:00\n",
      "req_task:  http://pitchfork.com/reviews/albums/23191-i/\n",
      "2017-05-10 16:14:32.775086 2017-05-08 05:00:00+00:00\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'connect_to_psql' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-11814140be7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m''' Run here to CRON '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Desktop/api_cred.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpf_croner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#pf_miner(cred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-540864fdeca9>\u001b[0m in \u001b[0;36mpf_croner\u001b[0;34m(credentials, req_lim)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpartition_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRON Completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-4a7bbf33cbca>\u001b[0m in \u001b[0;36mpartition_job\u001b[0;34m(source, credentials)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_cred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect_to_psql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     query = \"\"\"INSERT INTO reviews (artist, album, score, album_art, genre, label,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'connect_to_psql' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#We run the pf_miner.py first and hash out pf_croner. Subsequently we hash out pf_miner and just keep\n",
    "#running pf_croner.py which runs the entire pf_miner as well as the cron job every 24 hours.\n",
    "if __name__ == '__main__':\n",
    "    ''' Run here to CRON '''\n",
    "    cred = yaml.load(open(expanduser('~/Desktop/api_cred.yml')))\n",
    "    pf_croner(cred)\n",
    "    #pf_miner(cred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](images/DAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](images/DE_Eli_May10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
