{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Steps to do Structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The script \"make_db.py\" loads data from S3 using Spark, and parallel inserts the data into a RDS and PostgresSQL - including making table\n",
    "* Run \"spark submit make_db.py\"\n",
    "* Read raw data from S3\n",
    "* Create Spark dataFrames in 3 nf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDS - PostgreSQL\n",
    "\n",
    "* Read data into each table using a spark script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Please note, I am trying to run the code in Notebook here to make it look modular. The reason I am running into errors here is I have put some of the functions and other files in separate folders for proper book keeping. So from this location I would have to change the paths of the functions I am calling. Recommended way to run this--- All the scripts will run in PyCharm or Atom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-64808ef6cc92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# We run this on EMR. Make sure to install psycopg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "import json\n",
    "import yaml\n",
    "import psycopg2\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# We run this on EMR. Make sure to install psycopg2\n",
    "\n",
    "global my_cred\n",
    "\n",
    "\n",
    "def spark_job(sc, bucket_path):\n",
    "    rdd = sc.textFile(bucket_path)\n",
    "    print(\"Distributing jobs to partitions\")\n",
    "    rdd.foreachPartition(partition_job)\n",
    "    print(\"All partition jobs completed. Exiting...\")\n",
    "\n",
    "\n",
    "def partition_job(source, credentials=None):\n",
    "\n",
    "    if not credentials:\n",
    "        credentials = api_cred.value\n",
    "\n",
    "    conn, cur = connect_to_psql(credentials)\n",
    "\n",
    "    query = \"\"\"INSERT INTO reviews (artist, album, score, album_art, genre, label,\n",
    "                pub_date, abstract, review_content)\n",
    "                VALUES (%(artist)s, %(album)s, %(score)s, %(album_art)s, %(genre)s,\n",
    "                %(label)s, %(pub_date)s, %(abstract)s, %(review_content)s)\"\"\"\n",
    "\n",
    "    print(\"Starting job...\")\n",
    "\n",
    "    for item in source:\n",
    "\n",
    "        try:\n",
    "            if type(item) == unicode:\n",
    "                data = json.loads(item)\n",
    "            elif type(item) == dict:\n",
    "                data = item\n",
    "                print('Inserting album: ', data['album'])\n",
    "            cur.execute(query, data)\n",
    "        except:\n",
    "            conn.rollback()\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"Partition job completed.\")\n",
    "\n",
    "def make_table():\n",
    "\n",
    "    conn, cur = connect_to_psql()\n",
    "\n",
    "    query = \"\"\"CREATE TABLE IF NOT EXISTs reviews\n",
    "                (id SERIAL PRIMARY KEY, artist varchar(255), album varchar(255),\n",
    "                 score varchar(255), album_art varchar(255), genre varchar(255),\n",
    "                 label varchar(255), pub_date TIMESTAMP, abstract TEXT,\n",
    "                 featured_tracks varchar(255), review_content TEXT)\"\"\"\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "    except:\n",
    "        print(\"Something bad happenned while making tables...?\")\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"Finished creating reviews table.\")\n",
    "\n",
    "\n",
    "def connect_to_psql(credentials=None):\n",
    "\n",
    "    if not credentials:\n",
    "        credentials = my_cred.value\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting...\")\n",
    "        conn = psycopg2.connect(**credentials['rds'])\n",
    "        cur = conn.cursor()\n",
    "        print(\"Connection Established\")\n",
    "    except:\n",
    "        print(\"Error Establishing Connection (bad credentials?)\")\n",
    "        return False\n",
    "\n",
    "    return conn, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting...\n",
      "Connection Established\n"
     ]
    }
   ],
   "source": [
    "def connect_to_psql(credentials=None):\n",
    "\n",
    "\tif not credentials:\n",
    "\t\tcredentials = my_cred.value\n",
    "\n",
    "\ttry:\n",
    "\t\tprint(\"Connecting...\")\n",
    "\t\tconn = psycopg2.connect(**credentials['rds'])\n",
    "\t\tcur = conn.cursor()\n",
    "\t\tprint(\"Connection Established\")\n",
    "\texcept:\n",
    "\t\tprint(\"Error Establishing Connection (bad credentials?)\")\n",
    "\t\treturn False\n",
    "\n",
    "\treturn conn, cur\n",
    "\n",
    "\n",
    "my_cred = yaml.load(open(expanduser('~/Desktop/api_cred.yml')))\n",
    "conn, cur = connect_to_psql(my_cred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating table in 3NF form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The RDS database from which we query and get the data below, is basically the columns of data on artist name, genre, album, etc\n",
    "\n",
    "query = \"\"\"CREATE TABLE IF NOT EXISTs reviews\n",
    "                (id SERIAL PRIMARY KEY, artist varchar(255), album varchar(255),\n",
    "                 score varchar(255), album_art varchar(255), genre varchar(255),\n",
    "                 label varchar(255), pub_date TIMESTAMP, abstract TEXT,\n",
    "                 featured_tracks varchar(255), review_content TEXT)\"\"\"\n",
    "                 \n",
    "Since it is reviews we are parsing the table we obtain itself is in 3NF, so it does NOT serve anything extra useful to create multipe tables.\n",
    "\n",
    "We are extracting exactly the information we want using the script, as well as the query below.\n",
    "\n",
    "In this case the Primary key (PK) would be \"artist\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT album, artist, pub_date FROM reviews LIMIT 10;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Jamie, My Intentions Are Bass EP', '!!!', datetime.datetime(2010, 11, 1, 6, 0))\n",
      "('Konstellaatio', '', datetime.datetime(2014, 2, 27, 6, 0, 4))\n",
      "('As If', '!!!', datetime.datetime(2015, 10, 21, 6, 0))\n",
      "('New Heaven', '1,2,3', datetime.datetime(2011, 6, 21, 6, 0, 3))\n",
      "('11 11', '11 11', datetime.datetime(2014, 7, 29, 6, 0, 1))\n",
      "('120 Days II', '120 Days', datetime.datetime(2012, 4, 13, 6, 0, 4))\n",
      "('2 54', '2 54', datetime.datetime(2012, 6, 15, 6, 0, 3))\n",
      "('The Other I', '2 54', datetime.datetime(2014, 11, 11, 6, 0, 3))\n",
      "('Own Your Ghost', '13 & God', datetime.datetime(2011, 5, 17, 6, 0))\n",
      "('B.O.A.T.S II  Me Time', '2 Chainz', datetime.datetime(2013, 9, 18, 6, 0, 1))\n"
     ]
    }
   ],
   "source": [
    "for item in cur:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
